{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mak/anaconda3/envs/codehub/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mak/anaconda3/envs/codehub/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mak/anaconda3/envs/codehub/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mak/anaconda3/envs/codehub/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mak/anaconda3/envs/codehub/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mak/anaconda3/envs/codehub/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "#import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import sys\n",
    "from random import randint\n",
    "import datetime\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Removes an annoying Tensorflow warning\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "def createTrainMatrix(conversationFileName, wList, maxLen):\n",
    "    conversationDictionary = np.load(conversationFileName,allow_pickle=True).item()\n",
    "    numExamples = len(conversationDictionary)\n",
    "    xTrain = np.zeros((numExamples, maxLen), dtype='int32')\n",
    "    yTrain = np.zeros((numExamples, maxLen), dtype='int32')\n",
    "    for index,(key,value) in enumerate(conversationDictionary.items()):\n",
    "        # Will store integerized representation of strings here (initialized as padding)\n",
    "        encoderMessage = np.full((maxLen), wList.index('<pad>'), dtype='int32')\n",
    "        decoderMessage = np.full((maxLen), wList.index('<pad>'), dtype='int32')\n",
    "        # Getting all the individual words in the strings\n",
    "        keySplit = key.split()\n",
    "        valueSplit = value.split()\n",
    "        keyCount = len(keySplit)\n",
    "        valueCount = len(valueSplit)\n",
    "        # Throw out sequences that are too long or are empty\n",
    "        if (keyCount > (maxLen - 1) or valueCount > (maxLen - 1) or valueCount == 0 or keyCount == 0):\n",
    "            continue\n",
    "        # Integerize the encoder string\n",
    "        for keyIndex, word in enumerate(keySplit):\n",
    "            try:\n",
    "                encoderMessage[keyIndex] = wList.index(word)\n",
    "            except ValueError:\n",
    "                # TODO: This isnt really the right way to handle this scenario\n",
    "                encoderMessage[keyIndex] = 0\n",
    "        encoderMessage[keyIndex + 1] = wList.index('<EOS>')\n",
    "        # Integerize the decoder string\n",
    "        for valueIndex, word in enumerate(valueSplit):\n",
    "            try:\n",
    "                decoderMessage[valueIndex] = wList.index(word)\n",
    "            except ValueError:\n",
    "                decoderMessage[valueIndex] = 0\n",
    "        decoderMessage[valueIndex + 1] = wList.index('<EOS>')\n",
    "        xTrain[index] = encoderMessage\n",
    "        yTrain[index] = decoderMessage\n",
    "    # Remove rows with all zeros\n",
    "    yTrain = yTrain[~np.all(yTrain == 0, axis=1)]\n",
    "    xTrain = xTrain[~np.all(xTrain == 0, axis=1)]\n",
    "    numExamples = xTrain.shape[0]\n",
    "    return numExamples, xTrain, yTrain\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingBatch(localXTrain, localYTrain, localBatchSize, maxLen):\n",
    "    num = randint(0,numTrainingExamples - localBatchSize - 1)\n",
    "    arr = localXTrain[num:num + localBatchSize]\n",
    "    labels = localYTrain[num:num + localBatchSize]\n",
    "    # Reversing the order of encoder string apparently helps as per 2014 paper\n",
    "    reversedList = list(arr)\n",
    "    for index,example in enumerate(reversedList):\n",
    "        reversedList[index] = list(reversed(example))\n",
    "\n",
    "    # Lagged labels are for the training input into the decoder\n",
    "    laggedLabels = []\n",
    "    EOStokenIndex = wordList.index('<EOS>')\n",
    "    padTokenIndex = wordList.index('<pad>')\n",
    "    for example in labels:\n",
    "        eosFound = np.argwhere(example==EOStokenIndex)[0]\n",
    "        shiftedExample = np.roll(example,1)\n",
    "        shiftedExample[0] = EOStokenIndex\n",
    "        # The EOS token was already at the end, so no need for pad\n",
    "        if (eosFound != (maxLen - 1)):\n",
    "            shiftedExample[eosFound+1] = padTokenIndex\n",
    "        laggedLabels.append(shiftedExample)\n",
    "\n",
    "    # Need to transpose these\n",
    "    reversedList = np.asarray(reversedList).T.tolist()\n",
    "    labels = labels.T.tolist()\n",
    "    laggedLabels = np.asarray(laggedLabels).T.tolist()\n",
    "    return reversedList, labels, laggedLabels\n",
    "\n",
    "def translateToSentences(inputs, wList, encoder=False):\n",
    "    EOStokenIndex = wList.index('<EOS>')\n",
    "    padTokenIndex = wList.index('<pad>')\n",
    "    numStrings = len(inputs[0])\n",
    "    numLengthOfStrings = len(inputs)\n",
    "    listOfStrings = [''] * numStrings\n",
    "    for mySet in inputs:\n",
    "        for index,num in enumerate(mySet):\n",
    "            if (num != EOStokenIndex and num != padTokenIndex):\n",
    "                if (encoder):\n",
    "                    # Encodings are in reverse!\n",
    "                    listOfStrings[index] = wList[num] + \" \" + listOfStrings[index]\n",
    "                else:\n",
    "                    listOfStrings[index] = listOfStrings[index] + \" \" + wList[num]\n",
    "    listOfStrings = [string.strip() for string in listOfStrings]\n",
    "    return listOfStrings\n",
    "\n",
    "def getTestInput(inputMessage, wList, maxLen):\n",
    "    encoderMessage = np.full((maxLen), wList.index('<pad>'), dtype='int32')\n",
    "    inputSplit = inputMessage.lower().split()\n",
    "    for index,word in enumerate(inputSplit):\n",
    "        try:\n",
    "            encoderMessage[index] = wList.index(word)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    encoderMessage[index + 1] = wList.index('<EOS>')\n",
    "    encoderMessage = encoderMessage[::-1]\n",
    "    encoderMessageList=[]\n",
    "    for num in encoderMessage:\n",
    "        encoderMessageList.append([num])\n",
    "    return encoderMessageList\n",
    "\n",
    "def intList2Sentence(ids, wList):\n",
    "    EOStokenIndex = wList.index('<EOS>')\n",
    "    padTokenIndex = wList.index('<pad>')\n",
    "    myStr = \"\"\n",
    "    listOfResponses=[]\n",
    "    for num in ids:\n",
    "        if (num[0] == EOStokenIndex or num[0] == padTokenIndex):\n",
    "            listOfResponses.append(myStr)\n",
    "            myStr = \"\"\n",
    "        else:\n",
    "            myStr = myStr + wList[num[0]] + \" \"\n",
    "    if myStr:\n",
    "        listOfResponses.append(myStr)\n",
    "    listOfResponses = [i for i in listOfResponses if i]\n",
    "    return listOfResponses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training matrix created !\n",
      "padding index in wordlist 8866 eos at 8867\n",
      "Current Loss 9.091503 at i =  0\n",
      "Encoder Test String you wanna chill\n",
      "[\"I'm I'm \"]\n",
      "Current Loss 8.988535 at i =  50\n",
      "Encoder Test String Hi\n",
      "['9/10 9/10 ']\n",
      "Encoder Test String bob n vagene\n",
      "['9/10 9/10 9/10 ']\n",
      "Current Loss 6.918695 at i =  100\n",
      "Encoder Test String you wanna chill\n",
      "[]\n",
      "Encoder Test String bob n vagene\n",
      "[]\n",
      "Current Loss 5.3039618 at i =  150\n",
      "Encoder Test String hi there\n",
      "[]\n",
      "Encoder Test String bob n vagene\n",
      "[]\n",
      "Current Loss 3.9432845 at i =  200\n",
      "Encoder Test String Hi\n",
      "[]\n",
      "Encoder Test String Hi\n",
      "[]\n",
      "Current Loss 2.693292 at i =  250\n",
      "Encoder Test String hey ! who are you\n",
      "[]\n",
      "Encoder Test String you wanna chill\n",
      "[]\n",
      "Current Loss 2.3886664 at i =  300\n",
      "Encoder Test String you wanna chill\n",
      "[]\n",
      "Encoder Test String Hi\n",
      "[]\n",
      "Current Loss 2.4439323 at i =  350\n",
      "Encoder Test String Hi\n",
      "[]\n",
      "Encoder Test String hey ! who are you\n",
      "[]\n",
      "Current Loss 1.9808328 at i =  400\n",
      "Encoder Test String hey ! who are you\n",
      "[]\n",
      "Encoder Test String bob n vagene\n",
      "[]\n",
      "Current Loss 1.7446506 at i =  450\n",
      "Encoder Test String bob n vagene\n",
      "[]\n",
      "Encoder Test String Hi\n",
      "[]\n",
      "Current Loss 1.6128284 at i =  500\n",
      "Encoder Test String bob n vagene\n",
      "[]\n",
      "Encoder Test String bob n vagene\n",
      "[]\n",
      "Current Loss 1.8240616 at i =  550\n",
      "Encoder Test String you wanna chill\n",
      "[]\n",
      "Encoder Test String hi there\n",
      "[]\n",
      "Current Loss 1.3863289 at i =  600\n",
      "Encoder Test String hi there\n",
      "[]\n",
      "Encoder Test String you wanna chill\n",
      "[]\n",
      "Current Loss 1.7708185 at i =  650\n",
      "Encoder Test String Hi\n",
      "[]\n",
      "Encoder Test String bob n vagene\n",
      "[]\n",
      "Current Loss 1.9461485 at i =  700\n",
      "Encoder Test String hi there\n",
      "[]\n",
      "Encoder Test String hey ! who are you\n",
      "[]\n",
      "Current Loss 1.5178382 at i =  750\n",
      "Encoder Test String Hi\n",
      "[]\n",
      "Encoder Test String Hi\n",
      "[]\n",
      "Current Loss 1.4850318 at i =  800\n",
      "Encoder Test String you wanna chill\n",
      "[]\n",
      "Encoder Test String bob n vagene\n",
      "[]\n",
      "Current Loss 1.8816047 at i =  850\n",
      "Encoder Test String you wanna chill\n",
      "[]\n",
      "Encoder Test String hi there\n",
      "[]\n",
      "Current Loss 1.9668618 at i =  900\n",
      "Encoder Test String hi there\n",
      "[]\n",
      "Encoder Test String hi there\n",
      "[]\n",
      "Current Loss 1.8732418 at i =  950\n",
      "Encoder Test String hi there\n",
      "[]\n",
      "Encoder Test String you wanna chill\n",
      "[]\n",
      "Current Loss 1.3606375 at i =  1000\n",
      "Encoder Test String bob n vagene\n",
      "[]\n",
      "Encoder Test String bob n vagene\n",
      "[]\n",
      "Current Loss 1.5948406 at i =  1050\n",
      "Encoder Test String Hi\n",
      "[]\n",
      "Encoder Test String hey ! who are you\n",
      "[]\n",
      "Current Loss 1.2072676 at i =  1100\n",
      "Encoder Test String hi there\n",
      "[]\n",
      "Encoder Test String hey ! who are you\n",
      "[]\n",
      "Current Loss 1.1599311 at i =  1150\n",
      "Encoder Test String bob n vagene\n",
      "[]\n",
      "Encoder Test String bob n vagene\n",
      "[]\n",
      "Current Loss 1.7238173 at i =  1200\n",
      "Encoder Test String bob n vagene\n",
      "[]\n",
      "Encoder Test String hey ! who are you\n",
      "['9/10 ']\n",
      "Current Loss 1.2110722 at i =  1250\n",
      "Encoder Test String you wanna chill\n",
      "['9/10 ']\n",
      "Encoder Test String hi there\n",
      "['9/10 ']\n",
      "Current Loss 1.2062393 at i =  1300\n",
      "Encoder Test String bob n vagene\n",
      "['9/10 ']\n",
      "Encoder Test String Hi\n",
      "['9/10 ']\n",
      "Current Loss 1.5508662 at i =  1350\n",
      "Encoder Test String hey ! who are you\n",
      "['9/10 ']\n",
      "Encoder Test String you wanna chill\n",
      "['9/10 ']\n",
      "Current Loss 1.2913008 at i =  1400\n",
      "Encoder Test String you wanna chill\n",
      "['9/10 ']\n",
      "Encoder Test String hi there\n",
      "['9/10 ']\n",
      "Current Loss 1.7412846 at i =  1450\n",
      "Encoder Test String hi there\n",
      "['9/10 ']\n",
      "Encoder Test String you wanna chill\n",
      "['9/10 ']\n",
      "Current Loss 1.1922626 at i =  1500\n",
      "Encoder Test String bob n vagene\n",
      "['9/10 ']\n",
      "Encoder Test String you wanna chill\n",
      "['9/10 ']\n",
      "Current Loss 0.8660936 at i =  1550\n",
      "Encoder Test String bob n vagene\n",
      "['9/10 ']\n",
      "Encoder Test String Hi\n",
      "['9/10 ']\n",
      "Current Loss 1.6284851 at i =  1600\n",
      "Encoder Test String hey ! who are you\n",
      "['9/10 ']\n",
      "Encoder Test String you wanna chill\n",
      "['9/10 ']\n",
      "Current Loss 1.2675284 at i =  1650\n",
      "Encoder Test String you wanna chill\n",
      "['9/10 ']\n",
      "Encoder Test String bob n vagene\n",
      "['9/10 ']\n",
      "Current Loss 1.4114002 at i =  1700\n",
      "Encoder Test String you wanna chill\n",
      "['9/10 ']\n",
      "Encoder Test String hi there\n",
      "['9/10 ']\n",
      "Current Loss 1.1577998 at i =  1750\n",
      "Encoder Test String you wanna chill\n",
      "['9/10 ']\n",
      "Encoder Test String bob n vagene\n",
      "['9/10 ']\n",
      "Current Loss 1.7085278 at i =  1800\n",
      "Encoder Test String hey ! who are you\n",
      "['9/10 ']\n",
      "Encoder Test String you wanna chill\n",
      "['9/10 ']\n",
      "Current Loss 0.90520287 at i =  1850\n",
      "Encoder Test String hey ! who are you\n",
      "['9/10 ']\n",
      "Encoder Test String hi there\n",
      "['9/10 ']\n",
      "Current Loss 1.3535386 at i =  1900\n",
      "Encoder Test String bob n vagene\n",
      "['9/10 ']\n",
      "Encoder Test String you wanna chill\n",
      "['9/10 ']\n",
      "Current Loss 1.0539671 at i =  1950\n",
      "Encoder Test String bob n vagene\n",
      "['9/10 ']\n",
      "Encoder Test String Hi\n",
      "['9/10 ']\n",
      "Current Loss 1.412804 at i =  2000\n",
      "Encoder Test String Hi\n",
      "['9/10 ']\n",
      "Encoder Test String Hi\n",
      "['9/10 ']\n",
      "Current Loss 1.0270772 at i =  2050\n",
      "Encoder Test String you wanna chill\n",
      "['9/10 ']\n",
      "Encoder Test String you wanna chill\n",
      "['9/10 ']\n",
      "Current Loss 1.4731004 at i =  2100\n",
      "Encoder Test String hey ! who are you\n",
      "['9/10 ']\n",
      "Encoder Test String you wanna chill\n",
      "['9/10 ']\n",
      "Current Loss 1.6684438 at i =  2150\n",
      "Encoder Test String Hi\n",
      "['9/10 ']\n",
      "Encoder Test String bob n vagene\n",
      "['9/10 ']\n",
      "Current Loss 1.1787014 at i =  2200\n",
      "Encoder Test String Hi\n",
      "['9/10 ']\n",
      "Encoder Test String Hi\n",
      "['9/10 ']\n"
     ]
    }
   ],
   "source": [
    "#main model development\n",
    "#tf.compat.v1.disable_eager_execution()   #tfv2\n",
    "#disable_eager_execution()\n",
    "datarange = {'2015-01'}\n",
    "batchSize = 25\n",
    "maxEncoderLength = 95                                        #represents max length of input /output sentence\n",
    "maxDecoderLength = maxEncoderLength\n",
    "lstmUnits = 112\n",
    "embeddingDim = lstmUnits\n",
    "numLayersLSTM = 3\n",
    "numIterations = 15000\n",
    "for timeframe in datarange:\n",
    "    with open(\"wordList.txt\",\"rb\") as fp:\n",
    "        wordList = pickle.load(fp)\n",
    "    \n",
    "    wordList.append('<pad>')\n",
    "    wordList.append('<EOS>')\n",
    "    vocabSize = len(wordList)\n",
    "    #question = 'How many dimensions do you want your word vectors to be?: '\n",
    "    #wordVecDimensions = int(input(question))\n",
    "    #     if (os.path.isfile('embeddingMatrix.npy')):\n",
    "    #         wordVectors = np.load('embeddingMatrix.npy')\n",
    "    #         wordVecDimensions = wordVectors.shape[1]\n",
    "    \n",
    "    wordVecDimensions = 5\n",
    "    padVector = np.zeros((1,wordVecDimensions), dtype = 'int32')\n",
    "    EOSVector = np.zeros((1,wordVecDimensions), dtype = 'int32')\n",
    "    generateTrainDataFlag= True\n",
    "    if(generateTrainDataFlag):\n",
    "        numTrainingExamples, xTrain, yTrain = createTrainMatrix(\"./chatdata/comrepTrain{}.npy\".format(timeframe), wordList, maxEncoderLength)\n",
    "        np.save('seq2sexXTrain.npy',xTrain)\n",
    "        np.save('seq2sexYTrain.npy',yTrain)\n",
    "    else:   \n",
    "        if (os.path.isfile('seq2sexXTrain.npy') and os.path.isfile('seq2sexYTrain.npy')):\n",
    "            xTrain= np.load('seq2sexXTrain.npy')\n",
    "            yTrain= np.load('seq2sexYTrain.npy')\n",
    "            numTrainingExamples = xTrain.shape[0]\n",
    "        else:\n",
    "            print(\"Train Data not found ! please generate train data first.\")\n",
    "\n",
    "    \n",
    "    print(\"Training matrix created !\")\n",
    "    \n",
    "    print(\"padding index in wordlist\",wordList.index('<pad>'),\"eos at\",wordList.index('<EOS>'))\n",
    "    tfV2 = \"\"\"    encoderInputs = [tf.keras.backend.placeholder(dtype=tf.int32, shape=(None,)) for i in range(maxEncoderLength)]\n",
    "    decoderInputs = [tf.keras.backend.placeholder(dtype=tf.int32, shape=(None,)) for i in range(maxDecoderLength)]\n",
    "    decoderLabels = [tf.keras.backend.placeholder(dtype=tf.int32, shape=(None,)) for i in range(maxDecoderLength)]\n",
    "    feedPrevious = tf.keras.backend.placeholder(dtype=tf.bool)\n",
    "    #long short-term memory\n",
    "    encoderLstm = tf.compat.v1.nn.rnn_cell.LSTMCell(lstmUnits, state_is_tuple=True)\n",
    "    encoderLstmKeras = tf.keras.layers.LSTMCell(lstmUnits)\n",
    "    #     encoderLSTM = tf.nn.rnn_cell.MultiRNNCell([singleCell]*numLayersLSTM, state_is_tuple=True)\n",
    "    #^ for cloud training \n",
    "    decoderOutput, decoderFinalState = tfa.seq2seq.embedding_rnn_seq2seq(encoderInputs, decoderInputs, encoderLstm,vocabSize, vocabSize,embeddingDim,feedPrevious_previous=feedPrevious) \n",
    "    \n",
    "    #PREV:09032021  [tf.nn.seq2seq.embedding_rnn_seq2seq][tf.models.rnn.seq2seq.embedding_rnn_seq2seq]\n",
    "    #     decoderPrediction = tf.argmax(decoderOutput,2)\n",
    "    \n",
    "    #     lossWeights = [tf.ones_like(l,dtype=tf.float32) for l in decoderLabels]\n",
    "    #     loss = tfa.legacy_seq2seq.sequence_loss(decoderOutput, decoderLabels, lossWeights, vocabSize)\n",
    "    #     optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    \n",
    "    \"\"\"    \n",
    "    #tf v1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    encoderInputs = [tf.placeholder(tf.int32, shape=(None,)) for i in range(maxEncoderLength)]\n",
    "    decoderLabels = [tf.placeholder(tf.int32,shape=(None,)) for i in range(maxDecoderLength)]\n",
    "    decoderInputs = [tf.placeholder(tf.int32,shape=(None,)) for i in range(maxDecoderLength)]\n",
    "    feedPrevious = tf.placeholder(tf.bool)\n",
    "    encoderLSTM = tf.nn.rnn_cell.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "    \n",
    "    decoderOutputs, decoderFinalState = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(encoderInputs, decoderInputs, encoderLSTM,vocabSize,vocabSize,embedding_size= embeddingDim, feed_previous = feedPrevious)\n",
    "   \n",
    "    decoderPrediction = tf.argmax(decoderOutputs,2)\n",
    "    \n",
    "    lossWeights = [tf.ones_like(l,dtype= tf.float32) for l in decoderLabels]\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss(decoderOutputs,decoderLabels,lossWeights,vocabSize)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    #loading a saved model\n",
    "    #saver.restore(sess,tf.train.latest_checkpoint('models/'))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #forwarding result to tensorboard\n",
    "    tf.summary.scalar('loss',loss)\n",
    "    merged= tf.summary.merge_all()\n",
    "    logDir = \"tensorboard/\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"/\"\n",
    "    writer = tf.summary.FileWriter(logDir, sess.graph)\n",
    "     # test strings\n",
    "    encoderTestStrings = [\"Hi\",\"hey ! who are you\",\"hi there\",\"you wanna chill\",\"bob n vagene\"]\n",
    "    zeroVector = np.zeros((1),dtype= 'int32')\n",
    "    for i in range(numIterations):\n",
    "        encoderTrain, decodertargerTrain, decoderInputTrain, = getTrainingBatch(xTrain,yTrain, batchSize , maxEncoderLength)\n",
    "        feedDict = {encoderInputs[t]:encoderTrain[t] for t in range(maxEncoderLength)}\n",
    "        feedDict.update({decoderLabels[t]:decodertargerTrain[t] for t in range(maxDecoderLength)})\n",
    "        feedDict.update({decoderInputs[t]:decoderInputTrain[t] for t in range(maxDecoderLength)})\n",
    "        feedDict.update({feedPrevious:False})\n",
    "        curLoss ,_,pred = sess.run([loss, optimizer, decoderPrediction],feed_dict = feedDict)\n",
    "        if(i%50 == 0):\n",
    "            print(\"Current Loss\",curLoss, \"at i = \",i)\n",
    "            summary = sess.run(merged,feed_dict=feedDict)\n",
    "            writer.add_summary(summary,i)\n",
    "        if(i%25==0 and i!=0):\n",
    "            num = randint(0,len(encoderTestStrings)-1)\n",
    "            print(\"Encoder Test String\",encoderTestStrings[num])\n",
    "            inputVector = getTestInput(encoderTestStrings[num],wordList,maxEncoderLength)\n",
    "            feedDict = {encoderInputs[t]:inputVector[t] for t in range(maxEncoderLength)}\n",
    "            feedDict.update({decoderLabels[t]:zeroVector for t in range(maxDecoderLength)})\n",
    "            feedDict.update({decoderInputs[t]:zeroVector for t in range(maxDecoderLength)})\n",
    "            feedDict.update({feedPrevious: True})\n",
    "            ids = (sess.run(decoderPrediction,feed_dict= feedDict))\n",
    "            print(intList2Sentence(ids,wordList))\n",
    "        if(i%1000 == 0 and i!=0):\n",
    "            savePath= saver.save(sess,\"models/pretrained_seq2seq.ckpt\",global_step=i)\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
